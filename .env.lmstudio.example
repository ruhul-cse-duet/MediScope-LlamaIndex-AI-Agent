# ============================================================================
# MediScope - LM Studio Medical Configuration
# ============================================================================
# Copy this configuration to your .env file for local medical AI setup
# Using: LM Studio + qwen3-medical + Mini-InternVL2-1B-DA-Medical
# ============================================================================

# Application Settings
# ----------------------------------------------------------------------------
APP_NAME=MediScope
APP_VERSION=0.2.0
ENVIRONMENT=local
LOG_LEVEL=INFO
ALLOWED_ORIGINS=http://localhost:8000,http://127.0.0.1:8000

# Feature Flags
# ----------------------------------------------------------------------------
DEMO_MODE=false
ENABLE_FILE_LOGGING=true

# Upload Limits
# ----------------------------------------------------------------------------
MAX_UPLOAD_MB=20

# ============================================================================
# LM STUDIO CONFIGURATION (PRIMARY)
# ============================================================================
# 
# Prerequisites:
# 1. Download LM Studio from: https://lmstudio.ai/
# 2. Search and download: towardsinnovationlab/qwen3-medical-gguf
# 3. Load the model in LM Studio
# 4. Start the local server (default port: 1234)
#
# Model Variants (choose based on your RAM):
# - Q4_K_M: 6 GB RAM (recommended for most users)
# - Q6_K: 10 GB RAM (better quality)
# - Q8_0: 12 GB RAM (best quality)
# ============================================================================

# LLM Configuration - LM Studio
LLM_PROVIDER=lmstudio
LLM_MODEL=towardsinnovationlab/qwen3-medical-gguf
LMSTUDIO_URL=http://localhost:1234

# ============================================================================
# MEDICAL VISION MODEL CONFIGURATION
# ============================================================================
#
# Using Mini-InternVL2-1B-DA-Medical for medical image analysis
# This model will be automatically downloaded from HuggingFace on first use
# 
# Prerequisites:
# pip install transformers torch torchvision accelerate
#
# System Requirements:
# - First download: ~3 GB
# - RAM: 4 GB minimum (8 GB recommended)
# - GPU: Optional but recommended for faster inference
# ============================================================================

VISION_PROVIDER=internvl
VISION_MODEL=OpenGVLab/Mini-InternVL2-1B-DA-Medical

# ============================================================================
# OCR CONFIGURATION (Optional but Recommended)
# ============================================================================
#
# For extracting text from medical images before analysis
#
# Prerequisites for tesseract:
# Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki
# Add Tesseract to PATH
#
# Then: pip install pytesseract
# ============================================================================

OCR_PROVIDER=tesseract

# ============================================================================
# RAG CONFIGURATION (Optional but Recommended)
# ============================================================================
#
# Simple RAG: No dependencies, works out of the box
# LlamaIndex RAG: Better quality, requires installation
#
# For llamaindex:
# pip install llama-index llama-index-embeddings-huggingface
# ============================================================================

RAG_PROVIDER=simple

# ============================================================================
# SPEECH SERVICES (Optional)
# ============================================================================
#
# STT (Speech-to-Text) and TTS (Text-to-Speech)
# Enable if you want voice interaction
#
# For faster_whisper: pip install faster-whisper
# For gtts: pip install gtts
# ============================================================================

STT_PROVIDER=none
TTS_PROVIDER=none

# ============================================================================
# TIMEOUTS (Adjust based on your system performance)
# ============================================================================
#
# Increase these if you have a slower system or using larger models
# ============================================================================

LLM_TIMEOUT=120
STT_TIMEOUT=120
TTS_TIMEOUT=120
VISION_TIMEOUT=150
HTTP_TIMEOUT=80

# ============================================================================
# RETRY SETTINGS
# ============================================================================

MAX_RETRIES=3
RETRY_DELAY=2.0

# ============================================================================
# ALTERNATIVE CONFIGURATIONS
# ============================================================================

# ----------------------------------------------------------------------------
# Alternative 1: OpenAI + Local Vision
# ----------------------------------------------------------------------------
# Use this if you want cloud LLM but local vision processing
#
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o-mini
# OPENAI_API_KEY=your-openai-api-key-here
# VISION_PROVIDER=internvl
# VISION_MODEL=OpenGVLab/Mini-InternVL2-1B-DA-Medical
# ----------------------------------------------------------------------------

# ----------------------------------------------------------------------------
# Alternative 2: All Cloud (OpenAI)
# ----------------------------------------------------------------------------
# Use this if you want everything from OpenAI (costs money, sends data to cloud)
#
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o
# OPENAI_API_KEY=your-openai-api-key-here
# VISION_PROVIDER=none
# OCR_PROVIDER=none
# ----------------------------------------------------------------------------

# ----------------------------------------------------------------------------
# Alternative 3: vLLM Server (Advanced)
# ----------------------------------------------------------------------------
# Use this if you have a separate vLLM server running
#
# LLM_PROVIDER=vllm
# LLM_MODEL=your-model-name
# VLLM_URL=http://your-vllm-server:8000
# VISION_PROVIDER=vllm
# ----------------------------------------------------------------------------

# ============================================================================
# TROUBLESHOOTING
# ============================================================================
#
# If LM Studio connection fails:
# 1. Ensure LM Studio is running
# 2. Check server is started in "Local Server" tab
# 3. Verify URL is http://localhost:1234
# 4. Test in browser: http://localhost:1234/v1/models
#
# If vision model fails to load:
# 1. Check internet connection (first download)
# 2. Ensure 10+ GB free disk space
# 3. Install dependencies: pip install transformers torch
# 4. Check logs in logs/mediscope.log
#
# For detailed help:
# - See LMSTUDIO_SETUP.md (English)
# - See LMSTUDIO_SETUP_BANGLA.md (Bengali/বাংলা)
# - See TROUBLESHOOTING.md
# ============================================================================

# ============================================================================
# PERFORMANCE TIPS
# ============================================================================
#
# For better performance:
# 1. Use GPU if available (enable GPU offload in LM Studio)
# 2. Use Q4_K_M model variant for balance of speed/quality
# 3. Increase RAM if using larger models
# 4. Close other applications to free resources
# 5. Use SSD instead of HDD for faster model loading
#
# For better quality:
# 1. Use Q6_K or Q8_0 model variants
# 2. Increase context length in LM Studio
# 3. Enable RAG with llamaindex
# 4. Provide detailed medical context in queries
# ============================================================================

# ============================================================================
# PRIVACY & SECURITY
# ============================================================================
#
# Current configuration (LM Studio + InternVL):
# ✅ All data stays on your local machine
# ✅ No external API calls
# ✅ No data sent to cloud
# ✅ HIPAA-friendly when properly secured
# ✅ Works offline after initial model downloads
#
# Security recommendations:
# - Use firewall to block external access to LM Studio port
# - Don't expose LM Studio to public internet
# - Keep LM Studio and dependencies updated
# - Use VPN if remote access needed
# - Regular backups of configuration
# ============================================================================

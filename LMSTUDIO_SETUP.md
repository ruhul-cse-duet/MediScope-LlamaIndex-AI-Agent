# LM Studio Integration Guide
# MediScope AI Agent - Local Medical LLM Setup

## Overview

This guide shows you how to use **LM Studio** with the **qwen3-medical-gguf** model for local medical text generation, and **Mini-InternVL2-1B-DA-Medical** for vision tasks.

### Architecture
```
MediScope
‚îú‚îÄ‚îÄ Text Generation ‚Üí LM Studio (qwen3-medical-gguf)
‚îú‚îÄ‚îÄ Vision Tasks ‚Üí Hugging Face (Mini-InternVL2-1B-DA-Medical)
‚îú‚îÄ‚îÄ OCR ‚Üí Tesseract
‚îî‚îÄ‚îÄ RAG ‚Üí LlamaIndex
```

---

## Part 1: LM Studio Setup

### Step 1: Download LM Studio

1. Visit: https://lmstudio.ai/
2. Download LM Studio for Windows
3. Install LM Studio

### Step 2: Download Medical Model

1. **Open LM Studio**
2. Go to **"Search"** tab
3. Search for: `towardsinnovationlab/qwen3-medical-gguf`
4. Download the model:
   - Recommended: `qwen3-medical-Q4_K_M.gguf` (balanced quality/speed)
   - Alternative: `qwen3-medical-Q5_K_M.gguf` (higher quality, slower)
   - Fast option: `qwen3-medical-Q3_K_M.gguf` (faster, lower quality)

**Model Size Guide:**
- Q3_K_M: ~2.5 GB (Fastest, Good for testing)
- Q4_K_M: ~3.5 GB (Recommended - Best balance)
- Q5_K_M: ~4.5 GB (Higher quality)
- Q6_K: ~5.5 GB (Highest quality, slowest)

### Step 3: Load the Model

1. Go to **"Local Server"** tab in LM Studio
2. Click **"Select a model to load"**
3. Choose: `towardsinnovationlab/qwen3-medical-gguf`
4. Click **"Load Model"**
5. Wait for model to load (you'll see "Model loaded" message)

### Step 4: Start the Server

1. In **"Local Server"** tab:
   - **Port:** 1234 (default, don't change unless needed)
   - **CORS:** Enable CORS (check the box)
2. Click **"Start Server"**
3. Server should start on: `http://localhost:1234`

### Step 5: Test the Server

Open browser and go to:
```
http://localhost:1234/v1/models
```

You should see JSON response with model info.

---

## Part 2: MediScope Configuration

### Update .env File

Edit your `.env` file:

```bash
# ============================================================================
# MediScope Configuration for LM Studio + Vision Model
# ============================================================================

# LLM Provider - Using LM Studio for text generation
LLM_PROVIDER=lmstudio
LLM_MODEL=towardsinnovationlab/qwen3-medical-gguf
LMSTUDIO_URL=http://localhost:1234

# Vision Provider - Using InternVL for medical image analysis
VISION_PROVIDER=internvl
VISION_MODEL=OpenGVLab/Mini-InternVL2-1B-DA-Medical

# OCR Provider - For text extraction from images
OCR_PROVIDER=tesseract

# RAG Provider
RAG_PROVIDER=simple

# Other settings
DEMO_MODE=false
ENVIRONMENT=local
LOG_LEVEL=INFO
```

### Install Dependencies

```bash
cd backend
pip install -r requirements-ml.txt
```

This installs:
- `transformers` - For Mini-InternVL2 vision model
- `torch` - PyTorch for deep learning
- `pytesseract` - OCR engine
- `pillow` - Image processing
- And other ML dependencies

---

## Part 3: System Requirements

### For LM Studio:
- **RAM:** 8GB minimum (16GB recommended)
- **Storage:** 10GB free space
- **OS:** Windows 10/11
- **CPU:** Modern multi-core processor

### For Vision Model (Mini-InternVL2):
- **RAM:** 8GB minimum
- **VRAM:** 2GB GPU memory (or runs on CPU)
- **Storage:** 5GB for model cache

### Total System:
- **RAM:** 16GB recommended
- **Storage:** 15-20GB free
- **GPU:** Optional but recommended for vision tasks

---

## Part 4: Testing the Setup

### 1. Test LM Studio Separately

In LM Studio's chat interface:
```
User: What are the symptoms of diabetes?

Expected: Detailed medical response about diabetes symptoms
```

### 2. Test MediScope with LM Studio

```bash
# Start MediScope
cd backend
.venv\Scripts\activate
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Open browser: http://localhost:8000

Try:
```
User: What are the common symptoms of hypertension?

Expected: Medical response generated by qwen3-medical model
```

### 3. Test Vision Features

Upload an image (e.g., X-ray, medical chart) and ask:
```
Question: What does this medical image show?

Expected: Analysis from Mini-InternVL2-1B-DA-Medical model
```

---

## Part 5: Advanced Configuration

### Custom System Prompt (Optional)

Edit `backend/app/services/llm_service.py`:

```python
SYSTEM_PROMPT = (
    "You are a medical AI assistant specialized in Bangladeshi healthcare context. "
    "Provide accurate medical information in clear, simple language. "
    "Always emphasize when to seek professional medical care. "
    "Do not diagnose or prescribe medications."
)
```

### Model Parameters Tuning

In `.env`:
```bash
# LLM Settings
LLM_TIMEOUT=120        # Increase if responses are slow
MAX_RETRIES=3
RETRY_DELAY=1.0

# Vision Settings
VISION_TIMEOUT=150     # For complex image analysis
```

In LM Studio settings:
- **Temperature:** 0.3 (more focused) to 0.7 (more creative)
- **Max Tokens:** 512-1024 for medical responses
- **Top P:** 0.9 (good default)

---

## Part 6: Troubleshooting

### Issue 1: LM Studio Connection Failed

**Error:** `Cannot connect to LM Studio`

**Solutions:**
1. **Check LM Studio is running:**
   - Open LM Studio
   - Verify server is started (green indicator)
   - Check port is 1234

2. **Test connection manually:**
   ```bash
   curl http://localhost:1234/v1/models
   ```

3. **Check firewall:**
   - Allow LM Studio through Windows Firewall
   - Allow localhost connections

4. **Verify LMSTUDIO_URL in .env:**
   ```bash
   LMSTUDIO_URL=http://localhost:1234
   ```

### Issue 2: Model Not Loaded

**Error:** `Model not loaded in LM Studio`

**Solutions:**
1. Go to LM Studio ‚Üí Local Server tab
2. Click "Select a model to load"
3. Choose qwen3-medical model
4. Wait for "Model loaded" message
5. Then start server

### Issue 3: Slow Response Times

**Solutions:**
1. **Use smaller quantization:**
   - Switch from Q5_K_M to Q4_K_M or Q3_K_M

2. **Reduce max_tokens:**
   ```bash
   # In llm_service.py
   "max_tokens": 512  # Instead of 1000
   ```

3. **Enable GPU acceleration:**
   - LM Studio ‚Üí Settings ‚Üí GPU Layers
   - Increase GPU layers if you have GPU

### Issue 4: Vision Model Loading Failed

**Error:** `Failed to load vision model`

**Solutions:**
1. **Install required packages:**
   ```bash
   pip install transformers torch accelerate pillow
   ```

2. **Check GPU/CUDA:**
   ```bash
   # Test PyTorch
   python -c "import torch; print(torch.cuda.is_available())"
   ```

3. **Use CPU if no GPU:**
   - Model will automatically fall back to CPU
   - Will be slower but still works

4. **Clear cache and retry:**
   ```bash
   # Clear Hugging Face cache
   rm -rf ~/.cache/huggingface/
   ```

### Issue 5: Out of Memory

**Error:** `CUDA out of memory` or system freezes

**Solutions:**
1. **For LM Studio:**
   - Use smaller model (Q3_K_M instead of Q5_K_M)
   - Reduce context length in LM Studio settings

2. **For Vision Model:**
   - Set device_map="cpu" in vision_service.py
   - Process one image at a time

3. **Close other applications:**
   - Free up RAM
   - Close browsers, other ML tools

---

## Part 7: Production Deployment

### Option 1: Same Machine (Development)
```
[MediScope Backend] ‚Üê‚Üí [LM Studio localhost:1234]
                    ‚Üì
           [Vision Model (local)]
```

### Option 2: Separate Machines (Production)

**Server 1 (MediScope):**
```bash
LMSTUDIO_URL=http://192.168.1.100:1234
VISION_PROVIDER=internvl
```

**Server 2 (LM Studio):**
- Run LM Studio
- Configure to listen on 0.0.0.0:1234
- Allow network access in firewall

### Option 3: Docker Setup

**docker-compose.yml:**
```yaml
version: "3.9"

services:
  mediscope:
    build: .
    ports:
      - "8000:8000"
    environment:
      - LMSTUDIO_URL=http://host.docker.internal:1234
      - VISION_PROVIDER=internvl
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

---

## Part 8: Model Comparison

### Text Generation Models

| Model | Size | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| qwen3-medical Q3_K_M | 2.5GB | Fast | Good | Testing, Low RAM |
| qwen3-medical Q4_K_M | 3.5GB | Medium | Better | **Recommended** |
| qwen3-medical Q5_K_M | 4.5GB | Slow | Best | High accuracy needed |

### Vision Models

| Model | Size | Purpose | Local/Cloud |
|-------|------|---------|-------------|
| Mini-InternVL2-1B-DA-Medical | 2GB | Medical images | Local |
| GPT-4 Vision | N/A | General + Medical | Cloud (API) |

---

## Part 9: API Reference

### Chat Endpoint with LM Studio

**Request:**
```bash
POST http://localhost:8000/api/chat
Content-Type: application/json

{
  "message": "What are the symptoms of dengue fever?",
  "session_id": "user-123"
}
```

**Response:**
```json
{
  "session_id": "user-123",
  "message": "Dengue fever symptoms include: high fever (40¬∞C/104¬∞F), severe headache, pain behind eyes, joint and muscle pain, fatigue, nausea, vomiting, skin rash (appears 2-5 days after fever onset), and mild bleeding (nose bleeds, gum bleeding). Severe cases can develop dengue hemorrhagic fever. Seek immediate medical care if symptoms worsen.",
  "disclaimer": "This information is for education only...",
  "urgent_notice": null,
  "red_flag": false,
  "citations": [],
  "rag_context": null
}
```

### Vision Endpoint

**Request:**
```bash
POST http://localhost:8000/api/vision
Content-Type: multipart/form-data

file: [medical_xray.jpg]
question: "What abnormalities are visible in this X-ray?"
```

**Response:**
```json
{
  "ocr_text": "Patient: John Doe, Age: 45, Date: 2024-02-07",
  "answer": "The X-ray shows signs of pneumonia in the lower right lung field, with increased opacity and possible infiltration. Recommend clinical correlation and follow-up imaging."
}
```

---

## Part 10: Performance Optimization

### LM Studio Optimization

1. **GPU Settings:**
   ```
   LM Studio ‚Üí Settings ‚Üí GPU Layers
   - Set to maximum for your GPU
   - Monitor VRAM usage
   ```

2. **Context Length:**
   ```
   - Reduce to 2048 or 4096 for faster responses
   - Medical queries don't need huge context
   ```

3. **Batch Size:**
   ```
   - Keep at 1 for interactive chat
   - Increase for batch processing
   ```

### Vision Model Optimization

```python
# In vision_service.py
self._pipeline = pipeline(
    "image-text-to-text",
    model="OpenGVLab/Mini-InternVL2-1B-DA-Medical",
    trust_remote_code=True,
    device_map="auto",  # Use GPU if available
    torch_dtype=torch.float16,  # Use half precision
)
```

---

## Part 11: Monitoring & Logs

### Check LM Studio Logs

LM Studio console shows:
- Request received
- Token generation progress
- Response time
- Errors

### Check MediScope Logs

```bash
# Application logs
type logs\mediscope.log | findstr "LM Studio"

# Error logs
type logs\errors.log
```

### Performance Metrics

Monitor:
- **Response time:** Should be 2-10 seconds
- **Token/second:** Aim for 10-30 tokens/s
- **Memory usage:** Should stay under 70% RAM
- **GPU usage:** Should utilize GPU efficiently

---

## Part 12: FAQ

**Q: Can I use a different medical model?**  
A: Yes! In LM Studio, download any medical GGUF model and update `LLM_MODEL` in .env.

**Q: Does this work without internet?**  
A: Yes! Once models are downloaded, everything runs locally.

**Q: Can I use Bengali language?**  
A: qwen3 has some multilingual support. Test with Bengali prompts. For better Bengali support, try BanglaGPT or Bangla-LLaMA models.

**Q: How to update models?**  
A: In LM Studio, go to Search tab, download new version, then load it.

**Q: Can I use both OpenAI and LM Studio?**  
A: Not simultaneously in one instance. Choose one provider in .env. But you can easily switch by changing `LLM_PROVIDER`.

**Q: Is this HIPAA compliant?**  
A: Since models run locally, no data leaves your system. But you need proper deployment setup for HIPAA compliance.

---

## Part 13: Alternative Models

### For Text Generation (LM Studio)

1. **Meditron-7B** (Medical specialist)
   - More accurate for complex cases
   - Larger model (~7GB)

2. **BioMistral-7B** (Biomedical)
   - Strong scientific knowledge
   - Good for research queries

3. **Llama-3-8B-Medical** (General + Medical)
   - Balanced performance
   - Good multilingual support

### For Vision Tasks

1. **MedViT** - Medical image classification
2. **BiomedCLIP** - Biomedical vision-language
3. **GPT-4V** - Best quality (requires API)

---

## Part 14: Next Steps

After successful setup:

1. ‚úÖ **Test thoroughly** with various medical queries
2. ‚úÖ **Add medical documents** to RAG system
3. ‚úÖ **Customize prompts** for your use case
4. ‚úÖ **Monitor performance** and optimize
5. ‚úÖ **Deploy to production** (see PRODUCTION.md)

---

## Support & Resources

- **LM Studio Docs:** https://lmstudio.ai/docs
- **Qwen3 Model:** https://huggingface.co/towardsinnovationlab/qwen3-medical-gguf
- **InternVL Docs:** https://huggingface.co/OpenGVLab/Mini-InternVL2-1B-DA-Medical
- **MediScope Issues:** Create GitHub issue with details

---

## Summary Checklist

Before starting MediScope:

- [  ] LM Studio installed and running
- [  ] qwen3-medical-gguf model downloaded
- [  ] Model loaded in LM Studio
- [  ] Server started on port 1234
- [  ] .env configured with LMSTUDIO_URL
- [  ] ML dependencies installed
- [  ] Vision provider set to internvl
- [  ] Tested both text and vision endpoints

‚úÖ **All set? Start MediScope and enjoy local medical AI!** üéâ
